Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.
Assumptions of ANOVA:

Normality: The residuals should be approximately normally distributed.
Homogeneity of Variances: The variances of the groups being compared should be approximately equal.
Independence: Observations within each group should be independent of each other.
Violations and Impact:

Normality Violation: If the residuals are not normally distributed, it may affect the accuracy of p-values and confidence intervals.
Homogeneity of Variances Violation: Unequal variances may lead to unreliable F-statistics and affect the overall validity of ANOVA.
Independence Violation: If observations are not independent, it can lead to pseudoreplication and may invalidate the results.
Q2. What are the three types of ANOVA, and in what situations would each be used?
Three Types of ANOVA:

One-Way ANOVA: Used when comparing means of three or more groups.
Two-Way ANOVA: Used when there are two independent variables (factors) influencing the dependent variable.
Repeated Measures ANOVA: Used when the same subjects are used for each treatment (within-subjects design).
Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?
Partitioning of Variance:
In ANOVA, total variance is partitioned into different components:

Total Sum of Squares (SST): Variability of the dependent variable.
Explained Sum of Squares (SSE): Variability explained by the model.
Residual Sum of Squares (SSR): Unexplained variability.
Understanding this partitioning helps in assessing how much of the variability in the data is accounted for by the model.

Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?
python
Copy code
import scipy.stats as stats

# Example data for three groups
group1 = [10, 12, 15, 8, 11]
group2 = [14, 16, 18, 13, 15]
group3 = [9, 11, 10, 12, 8]

# Combine data from all groups
all_data = group1 + group2 + group3

# Calculate means
mean_all = np.mean(all_data)
mean_group1 = np.mean(group1)
mean_group2 = np.mean(group2)
mean_group3 = np.mean(group3)

# Calculate SST, SSE, SSR
SST = np.sum((all_data - mean_all)**2)
SSE = np.sum((group1 - mean_group1)**2) + np.sum((group2 - mean_group2)**2) + np.sum((group3 - mean_group3)**2)
SSR = SST - SSE
Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?
python
Copy code
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data for two factors (software programs and experience level)
# Assuming you have a DataFrame 'df' with columns 'time', 'program', and 'experience'
model = ols('time ~ program * experience', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Main effects
main_effect_program = anova_table['sum_sq']['program']
main_effect_experience = anova_table['sum_sq']['experience']

# Interaction effect
interaction_effect = anova_table['sum_sq']['program:experience']
Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?
Interpretation:
The F-statistic of 5.23 with a p-value of 0.02 suggests that there are significant differences between at least two of the groups. The null hypothesis (that all group means are equal) is rejected at a significance level of 0.05.

Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?
Handling Missing Data:

Complete Case Analysis: Exclude cases with missing data.
Imputation: Replace missing values with estimated values.
Consequences:

Complete case analysis may lead to biased results if missing data is not random.
Imputation methods introduce uncertainty and assumptions about the missing data mechanism.
Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.
Common Post-Hoc Tests:

Tukey's HSD: Used when comparing all pairs of means. Controls familywise error rate.
Bonferroni Correction: Adjusts significance level for multiple comparisons.
Sidak Correction: Similar to Bonferroni but less conservative.
Example:
If a one-way ANOVA indicates significant differences, a post-hoc test like Tukey's HSD can identify which specific groups differ from each other.

Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.
python
Copy code
import scipy.stats as stats

# Example data
diet_A = [2, 3, 4, 2, 3]
diet_B = [1, 2, 1, 3, 2]
diet_C = [3, 4, 3, 5, 4]

# One-way ANOVA
f_stat, p_value = stats.f_oneway(diet_A, diet_B, diet_C)

# Report results
print(f'F-statistic: {f_stat}')
print(f'p-value: {p_value}')

# Interpretation
if p_value < 0.05:
    print("Reject the null hypothesis. There are significant differences between the mean weight loss of the three diets.")
else:
    print("Fail to reject the null hypothesis. There are no significant differences between the mean weight loss of the three diets.")
Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.
python
Copy code
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data (assuming you have a DataFrame 'df' with columns 'time', 'program', and 'experience')
model = ols('time ~ program * experience', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

# Report results
print(anova_table)

### Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other.

```python
import scipy.stats as stats

# Example data (test scores for control and experimental groups)
control_group = [75, 80, 85, 78, 82, ...]  # scores for the control group
experimental_group = [82, 88, 92, 85, 90, ...]  # scores for the experimental group

# Two-sample t-test
t_stat, p_value = stats.ttest_ind(control_group, experimental_group)

# Report results
print(f'T-statistic: {t_stat}')
print(f'p-value: {p_value}')

# Interpretation
if p_value < 0.05:
    print("Reject the null hypothesis. There are significant differences in test scores between the control and experimental groups.")
else:
    print("Fail to reject the null hypothesis. There are no significant differences in test scores between the control and experimental groups.")
```

### Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other.

```python
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Example data (assuming you have a DataFrame 'df' with columns 'sales', 'store')
model = sm.OLS.from_formula('sales ~ store', data=df).fit()

# Repeated measures ANOVA
anova_table = sm.stats.anova_lm(model)

# Report results
print(anova_table)

# Follow-up post-hoc test (Tukey's HSD)
posthoc = pairwise_tukeyhsd(df['sales'], df['store'], alpha=0.05)

# Print the post-hoc results
print(posthoc)
```

